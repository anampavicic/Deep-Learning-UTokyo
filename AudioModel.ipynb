{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2afcaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Libraries for processing sounds\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c987561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478c2ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Failed to extract mel from data/augmented/Lion_1_modified.wav: Invalid or silent audio\n",
      "[WARNING] Failed to extract mel from data/sounds/Lion_1.wav: zero-size array to reduction operation maximum which has no identity\n"
     ]
    }
   ],
   "source": [
    "from AnimalSoundDataset import AnimalSoundDataset\n",
    "\n",
    "data_path = 'data/Animal_Sound_modified.csv'\n",
    "dataset_train = AnimalSoundDataset(data_path, split='train', split_ratio=0.8, seed=42)\n",
    "dataset_val = AnimalSoundDataset(data_path, split='val', split_ratio=0.8, seed=42)\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "loader = DataLoader(dataset_train, batch_size=len(dataset_train))\n",
    "x_train, y_train = next(iter(loader))\n",
    "\n",
    "loader = DataLoader(dataset_val, batch_size=len(dataset_val))\n",
    "x_val, y_val = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e430fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Blocks import Conv2DBlock\n",
    "from AnimalSoundDataset import AnimalSoundDataset\n",
    "from Utilities import Utilities\n",
    "\n",
    "# Libraries for processing sounds\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import random\n",
    "\n",
    "def get_activation(activation_str: str or None):\n",
    "\n",
    "    if activation_str == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation_str == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif activation_str == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif activation_str == \"linear\":\n",
    "        return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown activation function: {activation_str}\")\n",
    "\n",
    "\n",
    "class AudioModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hyperparameters):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.input_dim = hyperparameters['input_dim']\n",
    "        self.output_dim = hyperparameters['output_dim']\n",
    "        self.hidden_layers_size = hyperparameters['hidden_layers_size']\n",
    "        self.activation = hyperparameters['activation']\n",
    "        self.kernel_size_conv = hyperparameters['kernel_size_conv']\n",
    "        self.kernel_size_pool = hyperparameters['kernel_size_pool']\n",
    "        self.stride_conv = hyperparameters['stride_conv']\n",
    "        self.stride_pool = hyperparameters['stride_pool']\n",
    "        self.filters = hyperparameters['filters']\n",
    "        self.batch_normalization = hyperparameters['batch_normalization']\n",
    "        self.dropout_rate = hyperparameters['dropout_rate']\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Input Shape: (1, 128, target_width)\n",
    "        layer = Conv2DBlock(in_channels=self.input_dim, out_channels=self.filters[0], \n",
    "                            kernel_size=self.kernel_size_conv[0],stride=self.stride_conv[0],\n",
    "                            activation=get_activation(self.activation), \n",
    "                            batch_normalization=self.batch_normalization, dropout_rate=self.dropout_rate)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        layer = nn.MaxPool2d(kernel_size=self.kernel_size_pool[0], stride=self.stride_pool[0])\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        layer = Conv2DBlock(in_channels=self.filters[0], out_channels=self.filters[1],\n",
    "                            kernel_size=self.kernel_size_conv[1],stride=self.stride_conv[1],\n",
    "                            activation=get_activation(self.activation),\n",
    "                            batch_normalization=self.batch_normalization, dropout_rate=0.0)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        layer = nn.MaxPool2d(kernel_size=self.kernel_size_pool[1], stride=self.stride_pool[1])\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "        \n",
    "        # Flatten\n",
    "        self.layers.append(nn.Flatten())\n",
    "        \n",
    "        # 100 relus two times\n",
    "        # First FC layer\n",
    "        self.layers.append(nn.LazyLinear(self.hidden_layers_size))\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.layers.append(get_activation(self.activation))\n",
    "        \n",
    "        # Dropout\n",
    "        self.layers.append(nn.Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Second FC layer\n",
    "        self.layers.append(nn.LazyLinear(self.output_dim))\n",
    "        \n",
    "        #Relu Activation\n",
    "        self.layers.append(get_activation(self.activation))\n",
    "        \n",
    "        # Dropout\n",
    "        self.layers.append(nn.Dropout(self.dropout_rate)) \n",
    "        \n",
    "        # Softmax\n",
    "        self.layers.append(nn.Softmax(dim=1)) \n",
    "\n",
    "        self.classifier = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.classifier(x)\n",
    "        return y_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9de67511",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "n_classes = len(dataset_train.classes)\n",
    "hyperparameters = dict(input_dim=input_dim,\n",
    "                    output_dim=n_classes,\n",
    "                    hidden_layers_size=100,\n",
    "                    activation='relu',\n",
    "                    kernel_size_conv=[(57,6),(1,3)],\n",
    "                    kernel_size_pool=[(4,3),(1,3)],\n",
    "                    stride_conv=[(1,1),(1,1)],\n",
    "                    stride_pool=[(1,3),(1,3)],\n",
    "                    filters=[80,80],\n",
    "                    batch_normalization=False,\n",
    "                    dropout_rate=0.5,\n",
    "                    learning_rate=0.002,\n",
    "                    max_epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56d147b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from Utilities.Utilities import Utilities\n",
    "\n",
    "class AudioTrainer:\n",
    "    def __init__(self, model, train_dataset, val_dataset, hyperparameters, device='cpu'):\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=True)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=False)\n",
    "        self.max_epoch = self.hyperparameters['max_epoch']\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=self.hyperparameters['learning_rate'])\n",
    "        self.model.to(device)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.max_epoch):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0 #Running loss\n",
    "            total_accuracy = 0.0\n",
    "            n_batch = self.hyperparameters['batch_size']\n",
    "\n",
    "            for x_batch, y_batch in tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{self.max_epoch}\"):\n",
    "                x, y = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                # Forward\n",
    "                y_hat = self.model(x)\n",
    "                loss = self.criterion(y_hat, y)\n",
    "                # Backward\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                # Compute accuracy\n",
    "                batch_accuracy = Utilities.compute_accuracy(y, y_hat)\n",
    "                total_accuracy += batch_accuracy\n",
    "\n",
    "            train_loss = total_loss / n_batch\n",
    "            train_accuracy = total_accuracy / n_batch\n",
    "\n",
    "            val_acc, val_loss = self.evaluate()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: \"\n",
    "                  f\"Train Loss: {train_loss/len(self.train_loader):.4f}, \"\n",
    "                  f\"Train Acc: {train_accuracy:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        n_batch = self.hyperparameters['batch_size']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in self.val_loader:\n",
    "                x, y = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                # Forward\n",
    "                y_hat = self.model(x)\n",
    "                loss = self.criterion(y_hat, y)\n",
    "                total_loss += loss.item()\n",
    "                # Compute accuracy\n",
    "                batch_accuracy = Utilities.compute_accuracy(y, y_hat)\n",
    "                total_accuracy += batch_accuracy\n",
    "            valid_loss = total_loss / n_batch\n",
    "            valid_accuracy = total_accuracy / n_batch\n",
    "\n",
    "        return valid_accuracy, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e16dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AudioModel(hyperparameters=hyperparameters)\n",
    "# hyperparameters['batch_size'] = 128\n",
    "\n",
    "# trainer = AudioTrainer(model, dataset_train, dataset_val, hyperparameters, device=device)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4683fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-41.1521, -39.5143, -36.1628,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [-47.2608, -57.9575, -49.7822,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [-54.6373, -56.1950, -54.1422,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         ...,\n",
      "         [-80.0000, -80.0000, -80.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [-80.0000, -80.0000, -80.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "         [-80.0000, -80.0000, -80.0000,  ...,   0.0000,   0.0000,   0.0000]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anampavicic/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/conv.py:549: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1037.)\n",
      "  return F.conv2d(\n"
     ]
    }
   ],
   "source": [
    "model = AudioModel(hyperparameters).to(device)\n",
    "print(x_val[0])\n",
    "y_hat = model(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "401dd344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([260, 13])\n"
     ]
    }
   ],
   "source": [
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b7102",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

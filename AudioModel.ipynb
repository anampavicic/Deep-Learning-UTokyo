{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2afcaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Libraries for processing sounds\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c987561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478c2ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Failed to extract mel from data/sounds/Lion_1.wav: zero-size array to reduction operation maximum which has no identity\n"
     ]
    }
   ],
   "source": [
    "from AnimalSoundDataset import AnimalSoundDataset\n",
    "\n",
    "data_path = 'data/Animal_Sound_reduced.csv'\n",
    "dataset_train = AnimalSoundDataset(data_path, split='train', split_ratio=0.8, seed=42)\n",
    "dataset_val = AnimalSoundDataset(data_path, split='val', split_ratio=0.8, seed=42)\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "loader = DataLoader(dataset_train, batch_size=len(dataset_train))\n",
    "x_train, y_train = next(iter(loader))\n",
    "\n",
    "loader = DataLoader(dataset_val, batch_size=len(dataset_val))\n",
    "x_val, y_val = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee8710f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572b8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "n_classes = len(dataset_train.classes)\n",
    "hyperparameters = dict(input_dim=input_dim,\n",
    "                    output_dim=n_classes,\n",
    "                    hidden_layers_size=100,\n",
    "                    activation='relu',\n",
    "                    kernel_size_conv=[(57,6),(1,3)],\n",
    "                    kernel_size_pool=[(4,3),(1,3)],\n",
    "                    stride_conv=[(1,1),(1,1)],\n",
    "                    stride_pool=[(1,3),(1,3)],\n",
    "                    filters=[80,80],\n",
    "                    batch_normalization=False,\n",
    "                    dropout_rate=0.5,\n",
    "                    learning_rate=0.002,\n",
    "                    batch_size=len(dataset_val),\n",
    "                    max_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e69f694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anampavicic/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/conv.py:549: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1037.)\n",
      "  return F.conv2d(\n"
     ]
    }
   ],
   "source": [
    "from ClassesML.AudioModel import AudioModel\n",
    "model = AudioModel(hyperparameters).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05295968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "AudioModel                               --\n",
      "├─ModuleList: 1-1                        --\n",
      "│    └─Conv2DBlock: 2-1                  --\n",
      "│    │    └─Conv2d: 3-1                  27,440\n",
      "│    │    └─ReLU: 3-2                    --\n",
      "│    │    └─Dropout: 3-3                 --\n",
      "│    └─MaxPool2d: 2-2                    --\n",
      "│    └─Conv2DBlock: 2-3                  --\n",
      "│    │    └─Conv2d: 3-4                  19,280\n",
      "│    │    └─ReLU: 3-5                    --\n",
      "│    │    └─Dropout: 3-6                 --\n",
      "│    └─MaxPool2d: 2-4                    --\n",
      "│    └─Flatten: 2-5                      --\n",
      "│    └─Linear: 2-6                       44,000,100\n",
      "│    └─ReLU: 2-7                         --\n",
      "│    └─Dropout: 2-8                      --\n",
      "│    └─LazyLinear: 2-9                   --\n",
      "│    └─ReLU: 2-10                        --\n",
      "│    └─Dropout: 2-11                     --\n",
      "├─Sequential: 1-2                        44,046,820\n",
      "│    └─Conv2DBlock: 2-12                 (recursive)\n",
      "│    │    └─Conv2d: 3-7                  (recursive)\n",
      "│    │    └─ReLU: 3-8                    --\n",
      "│    │    └─Dropout: 3-9                 --\n",
      "│    └─MaxPool2d: 2-13                   --\n",
      "│    └─Conv2DBlock: 2-14                 (recursive)\n",
      "│    │    └─Conv2d: 3-10                 (recursive)\n",
      "│    │    └─ReLU: 3-11                   --\n",
      "│    │    └─Dropout: 3-12                --\n",
      "│    └─MaxPool2d: 2-15                   --\n",
      "│    └─Flatten: 2-16                     --\n",
      "│    └─Linear: 2-17                      (recursive)\n",
      "│    └─ReLU: 2-18                        --\n",
      "│    └─Dropout: 2-19                     --\n",
      "│    └─LazyLinear: 2-20                  --\n",
      "│    └─ReLU: 2-21                        --\n",
      "│    └─Dropout: 2-22                     --\n",
      "=================================================================\n",
      "Total params: 88,093,640\n",
      "Trainable params: 88,093,640\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973cc7ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70ff916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "model = AudioModel(hyperparameters).to(device)\n",
    "y_hat = model(x_val)\n",
    "# those are not probabilities, but logits\n",
    "y_hat\n",
    "# to get probabilities, we need to apply softmax\n",
    "y_prob = F.softmax(y_hat, dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98183169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    \n",
    "    train_accuracy_dict = {}\n",
    "    valid_accuracy_dict = {}\n",
    "    \n",
    "    for epoch in range(hyperparameters['max_epochs']):\n",
    "        # Train\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        # it is a good practice to split the dataset into batches\n",
    "        # as our computer can not handle everything at once\n",
    "        # this is only if there is a lot of data\n",
    "        n_batch = len(x_train)\n",
    "        for n in range(n_batch):\n",
    "            # you need to send the data to some device (GPU or CPU depending on what we have)\n",
    "            # or else we will get an error\n",
    "            x = x_train[n].to(device)\n",
    "            y = y_train[n].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat = model(x)\n",
    "            loss = scope.criterion(y_hat, y)\n",
    "\n",
    "            # Backward pass\n",
    "            self.scope.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.scope.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            batch_accuracy = Utilities.compute_accuracy(y, y_hat)\n",
    "            total_accuracy += batch_accuracy\n",
    "\n",
    "        train_loss = total_loss / n_batch\n",
    "        train_accuracy = total_accuracy / n_batch\n",
    "        print(\"Epoch: \", str(epoch + 1) + \"/\" + str(self.hyperparameters['max_epochs']))\n",
    "        print(\"Train Loss: \", str(train_loss) + \" - Training Accuracy: \" + str(train_accuracy))\n",
    "\n",
    "        # Validation\n",
    "    \n",
    "        # very important to set the model to eval mode\n",
    "        # we do not want to update the weights\n",
    "        # we want the current state of the model\n",
    "        self.model.eval() \n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        n_batch = len(self.x_valid)\n",
    "        for n in range(n_batch):\n",
    "            x = self.x_valid[n].to(self.device)\n",
    "            y = self.y_valid[n].to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat = self.model(x)\n",
    "            loss = self.scope.criterion(y_hat, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            batch_accuracy = Utilities.compute_accuracy(y, y_hat)\n",
    "            total_accuracy += batch_accuracy\n",
    "\n",
    "        valid_loss = total_loss / n_batch\n",
    "        valid_accuracy = total_accuracy / n_batch\n",
    "        print(\"Epoch: \", str(epoch + 1) + \"/\" + str(self.hyperparameters['max_epochs']))\n",
    "        print(\"Validation Loss: \", str(valid_loss) + \" - Validation Accuracy: \" + str(valid_accuracy))\n",
    "        train_accuracy_dict[epoch] = train_accuracy\n",
    "        valid_accuracy_dict[epoch] = valid_accuracy\n",
    "\n",
    "        # add the learning rate scheduler\n",
    "        if self.scope.scheduler:\n",
    "            validation_metric = valid_accuracy\n",
    "            old_lr = self.scope.optimizer.param_groups[0]['lr']\n",
    "            self.scope.scheduler.step(validation_metric)\n",
    "            new_lr = self.scope.optimizer.param_groups[0]['lr']\n",
    "            if old_lr != new_lr:\n",
    "                print(f\"Learning rate changed from {old_lr} to {new_lr}\")\n",
    "        \n",
    "        # add the early stopping mechanism\n",
    "        if self.scope.early_stopper:\n",
    "            validation_metric = valid_accuracy\n",
    "            keep_training = self.scope.early_stopper.set(\n",
    "                model=self.model,\n",
    "                epoch= epoch,\n",
    "                metric_epoch=validation_metric\n",
    "            )\n",
    "            if not keep_training:\n",
    "                break\n",
    "\n",
    "            if not keep_training:\n",
    "                break\n",
    "\n",
    "            \n",
    "    train_accuracy_list = [train_accuracy_dict[i] for i in train_accuracy_dict.keys()]\n",
    "    valid_accuracy_list = [valid_accuracy_dict[i] for i in valid_accuracy_dict.keys()]\n",
    "    return train_accuracy_list, valid_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1b118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Failed to extract mel from data/sounds/Lion_1.wav: zero-size array to reduction operation maximum which has no identity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 14.37485506, Train Acc: 0.73964497, Val Loss: 0.10593927, Val Acc: 0.29585799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  50%|█████     | 2/4 [00:03<00:03,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Failed to extract mel from data/sounds/Lion_1.wav: zero-size array to reduction operation maximum which has no identity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 4/4 [00:06<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.98137984, Train Acc: 1.18343195, Val Loss: 0.09865190, Val Acc: 0.44378698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Failed to extract mel from data/sounds/Lion_1.wav: zero-size array to reduction operation maximum which has no identity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.56099426, Train Acc: 1.47928994, Val Loss: 0.09865190, Val Acc: 0.44378698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Failed to extract mel from data/sounds/Lion_1.wav: zero-size array to reduction operation maximum which has no identity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.22829280, Train Acc: 0.88757396, Val Loss: 0.09865190, Val Acc: 0.44378698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Failed to extract mel from data/sounds/Lion_1.wav: zero-size array to reduction operation maximum which has no identity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 4/4 [00:06<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.11604216, Train Acc: 0.73964497, Val Loss: 0.09865190, Val Acc: 0.44378698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mClassesML\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mAudioTrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioTrainer\n\u001b[32m      2\u001b[39m trainer = AudioTrainer(model, dataset_train, dataset_val, hyperparameters, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/UTokyo_dev/deep_learning/Project/Deep-Learning-UTokyo/ClassesML/AudioTrainer.py:29\u001b[39m, in \u001b[36mAudioTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     27\u001b[39m x, y = x_batch.to(\u001b[38;5;28mself\u001b[39m.device), y_batch.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m y_hat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(y_hat, y)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/UTokyo_dev/deep_learning/Project/Deep-Learning-UTokyo/ClassesML/AudioModel.py:105\u001b[39m, in \u001b[36mAudioModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     y_hat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/UTokyo_dev/deep_learning/Project/Deep-Learning-UTokyo/ClassesML/Blocks.py:33\u001b[39m, in \u001b[36mConv2DBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.activation:\n\u001b[32m     32\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.activation(x)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/modules/dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DLClass/lib/python3.11/site-packages/torch/nn/functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from ClassesML.AudioTrainer import AudioTrainer\n",
    "trainer = AudioTrainer(model, dataset_train, dataset_val, hyperparameters, device=device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b7102",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

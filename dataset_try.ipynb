{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da21b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/Animal_Sound.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b8a377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sounds/Lion_12.wav'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['path'] = df['name'].apply(lambda x: f\"data/sounds/{x}\")\n",
    "df['path'][0]\n",
    "audio_path = df['path'][3]\n",
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a7fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.signal import resample\n",
    "\n",
    "def extract_and_stack_spectrograms(s, sr, window_sizes, T_target=None, F_target=None):\n",
    "    \"\"\"\n",
    "    Extract multi-resolution spectrograms, project to same time dimension,\n",
    "    and stack into a single 3D tensor (K, T, F).\n",
    "\n",
    "    Parameters:\n",
    "    - s: np.ndarray, audio waveform\n",
    "    - sr: int, sample rate\n",
    "    - window_sizes: list of int, FFT window sizes\n",
    "    - T_target: int or None, target time frames (if None, use max across resolutions)\n",
    "    - F_target: int or None, target frequency bins (if None, use min across resolutions)\n",
    "\n",
    "    Returns:\n",
    "    - xMR_stacked: np.ndarray of shape (K, T_target, F_target)\n",
    "    \"\"\"\n",
    "    spectrograms = []\n",
    "    T_list = []\n",
    "    F_list = []\n",
    "\n",
    "    # Step 1: Extract STFT magnitudes\n",
    "    for ω in window_sizes:\n",
    "        hop_length = ω // 4\n",
    "        S = librosa.stft(s, n_fft=ω, hop_length=hop_length, win_length=ω)\n",
    "        S_mag = np.abs(S).T  # Shape: (T_i, F_i)\n",
    "        spectrograms.append(S_mag)\n",
    "        T_list.append(S_mag.shape[0])\n",
    "        F_list.append(S_mag.shape[1])\n",
    "\n",
    "    # Set target dimensions\n",
    "    T_target = T_target or max(T_list)\n",
    "    F_target = F_target or min(F_list)  # To ensure all can be trimmed safely\n",
    "\n",
    "    # Step 2–4: Resample time & trim freq, then stack\n",
    "    processed = []\n",
    "    for spec in spectrograms:\n",
    "        # Resample along time axis to T_target\n",
    "        spec_resampled = resample(spec, T_target, axis=0)\n",
    "\n",
    "        # Trim or pad frequency axis to F_target\n",
    "        if spec_resampled.shape[1] > F_target:\n",
    "            spec_trimmed = spec_resampled[:, :F_target]\n",
    "        else:\n",
    "            pad_width = F_target - spec_resampled.shape[1]\n",
    "            spec_trimmed = np.pad(spec_resampled, ((0, 0), (0, pad_width)))\n",
    "\n",
    "        processed.append(spec_trimmed)\n",
    "\n",
    "    # Stack to shape (K, T_target, F_target)\n",
    "    xMR_stacked = np.stack(processed, axis=0)\n",
    "    return xMR_stacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b18262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2723, 129)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torch\n",
    "\n",
    "# lead file\n",
    "s, sr = librosa.load(audio_path, sr=22050)\n",
    "\n",
    "# # We propose to extract spectrograms of different temporal resolutions of FFT\n",
    "window_sizes = [256, 512, 1024]\n",
    "\n",
    "xMR = extract_and_stack_spectrograms(s, sr, window_sizes)\n",
    "\n",
    "print(xMR.shape)  # (K=3, T, F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86edcbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage\n",
    "def extract_xMR_xMRMF(file_path, \n",
    "                      sr=22050, \n",
    "                      window_sizes=[32, 64, 128], \n",
    "                      hop_length=256, \n",
    "                      target_T=None,\n",
    "                      target_F=128):  # FIX: Added target_F to unify frequency bins\n",
    "    y, _ = librosa.load(file_path, sr=sr)\n",
    "    \n",
    "    xMR_list = []\n",
    "    mel_list = []\n",
    "    T_list = []\n",
    "\n",
    "    for win_size in window_sizes:\n",
    "        # Raw FFT-based spectrogram\n",
    "        S_complex = librosa.stft(y, n_fft=win_size, hop_length=hop_length, window='hann')\n",
    "        S_mag = np.abs(S_complex)\n",
    "        S_db = librosa.amplitude_to_db(S_mag, ref=np.max)\n",
    "\n",
    "        # Mel-filtered version (use same freq bins as raw FFT for now)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=win_size,\n",
    "                                                  hop_length=hop_length, n_mels=S_db.shape[0],\n",
    "                                                  window='hann')\n",
    "        mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        xMR_list.append(S_db)\n",
    "        mel_list.append(mel_db)\n",
    "        T_list.append(S_db.shape[1])\n",
    "\n",
    "    # Determine common time length\n",
    "    if target_T is None:\n",
    "        target_T = max(T_list)\n",
    "\n",
    "    xMR_aligned = []\n",
    "    mel_aligned = []\n",
    "\n",
    "    for xmr, mel in zip(xMR_list, mel_list):\n",
    "        # Resample time (axis=1) and frequency (axis=0) to match target_T and target_F\n",
    "        xmr_resampled = scipy.ndimage.zoom(xmr, (target_F / xmr.shape[0], target_T / xmr.shape[1]), order=1)\n",
    "        mel_resampled = scipy.ndimage.zoom(mel, (target_F / mel.shape[0], target_T / mel.shape[1]), order=1)\n",
    "\n",
    "        xMR_aligned.append(xmr_resampled[..., np.newaxis])  # Shape: (F, T, 1)\n",
    "        mel_aligned.append(mel_resampled[..., np.newaxis])  # Shape: (F, T, 1)\n",
    "\n",
    "    # Now all shapes are (target_F, target_T, 1), so we can stack\n",
    "    xMR = np.stack(xMR_aligned, axis=1).transpose(2, 1, 0, 3)   # (F, K, T, 1) → (T, K, F, 1)\n",
    "    mel = np.stack(mel_aligned, axis=1).transpose(2, 1, 0, 3)   # (F, K, T, 1) → (T, K, F, 1)\n",
    "\n",
    "    xMRMF = np.concatenate([mel, xMR], axis=-1)  # (T, K, F, 2)\n",
    "\n",
    "    return xMR, xMRMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775f3cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anampavicic/miniconda3/envs/DLClass/lib/python3.11/site-packages/librosa/feature/spectral.py:2148: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(681, 3, 128, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "import torch\n",
    "\n",
    "xMR, xMRMF = extract_xMR_xMRMF(audio_path)\n",
    "\n",
    "\n",
    "xMRMF.shape  # Should be (T, K, F, 2) where K is number of resolutions, T is time frames, F is frequency bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a1d6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def prepare_xMRMF_for_patching(xMRMF_np, batch_first=True):\n",
    "    \"\"\"\n",
    "    Convert xMRMF from shape (T, K, F, 2) → (B, T, F, R)\n",
    "    \n",
    "    Args:\n",
    "        xMRMF_np: numpy array of shape (T, K, F, 2)\n",
    "        batch_first: whether to return shape (B, T, F, R) (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor of shape (B, T, F, R)\n",
    "    \"\"\"\n",
    "    # xMRMF_np: (T, K, F, 2)\n",
    "    T, K, F, Filt = xMRMF_np.shape\n",
    "    R = K * Filt\n",
    "    \n",
    "    # Reshape (T, K, F, 2) → (T, F, R)\n",
    "    xMRMF_np = xMRMF_np.transpose(0, 2, 1, 3)  # (T, F, K, 2)\n",
    "    xMRMF_np = xMRMF_np.reshape(T, F, R)      # (T, F, R)\n",
    "\n",
    "    # Add batch dimension: (B=1, T, F, R)\n",
    "    xMRMF_tensor = torch.tensor(xMRMF_np, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    if not batch_first:\n",
    "        xMRMF_tensor = xMRMF_tensor.permute(1, 0, 2, 3)  # (T, B, F, R)\n",
    "\n",
    "    return xMRMF_tensor  # Shape: (B, T, F, R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8307aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "xMRMF_tensor = prepare_xMRMF_for_patching(xMRMF, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d80f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 681, 128, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xMRMF_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1414b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, csv_path, sr=22050, window_sizes=[32, 64, 128],\n",
    "                 target_T=128, target_F=128, test_size=0.2, random_state=42):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df['path'] = df['name'].apply(lambda x: f\"data/sounds/{x}\")\n",
    "        self.df['label'] = df['name'].str.lower().str.split(\"_\").str[0]\n",
    "        self.sr = sr\n",
    "        self.window_sizes = window_sizes\n",
    "        self.target_T = target_T\n",
    "        self.target_F = target_F\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.label_to_index = {label: i for i, label in enumerate(sorted(self.df['label'].unique()))}\n",
    "\n",
    "    def extract_features_and_labels(self, paths, labels):\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        for path, label in tqdm(zip(paths, labels), total=len(paths), desc=\"Extracting xMRMF features\"):\n",
    "            try:\n",
    "                _, xMRMF_np = extract_xMR_xMRMF(path,\n",
    "                                                sr=self.sr,\n",
    "                                                window_sizes=self.window_sizes,\n",
    "                                                target_T=self.target_T,\n",
    "                                                target_F=self.target_F)\n",
    "                xMRMF_tensor = prepare_xMRMF_for_patching(xMRMF_np)  # (1, T, F, R)\n",
    "                data.append(xMRMF_tensor.squeeze(0))  # Remove batch dim: (T, F, R)\n",
    "                targets.append(self.label_to_index[label])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to process {path}: {e}\")\n",
    "\n",
    "        return torch.stack(data), torch.tensor(targets)\n",
    "\n",
    "    def load_dataset(self):\n",
    "        # Split train/val\n",
    "        train_df, val_df = train_test_split(self.df, test_size=self.test_size, random_state=self.random_state, stratify=self.df['label'])\n",
    "\n",
    "        train_data, train_labels = self.extract_features_and_labels(train_df['path'], train_df['label'])\n",
    "        val_data, val_labels = self.extract_features_and_labels(val_df['path'], val_df['label'])\n",
    "\n",
    "        train_dataset = [train_data, train_labels]\n",
    "        val_dataset = [val_data, val_labels]\n",
    "\n",
    "        input_dim = train_data.shape[1:]  # (T, F, R)\n",
    "        n_classes = len(self.label_to_index)\n",
    "\n",
    "        return train_dataset, val_dataset, input_dim, n_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a880a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = DatasetLoader('data/Animal_Sound.csv')\n",
    "# train_dataset, val_dataset, input_dim, n_classes = data.load_dataset()\n",
    "# print(f\"Train dataset shape: {train_dataset[0].shape}, Labels shape: {train_dataset[1].shape}\")\n",
    "# print(f\"Validation dataset shape: {val_dataset[0].shape}, Labels shape: {val_dataset[1].shape}\")\n",
    "# print(f\"Input dimension: {input_dim}, Number of classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a499587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, patch_size=(8, 8), in_channels=6, embed_dim=96):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F, R) → (B, R, T, F)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.proj(x)  # (B, embed_dim, T', F')\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, N_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding3D(nn.Module):\n",
    "    def __init__(self, T, F, R, embed_dim):\n",
    "        super().__init__()\n",
    "        self.time_emb = nn.Parameter(torch.randn(T, embed_dim))\n",
    "        self.freq_emb = nn.Parameter(torch.randn(F, embed_dim))\n",
    "        self.res_emb = nn.Parameter(torch.randn(R, embed_dim))\n",
    "        self.proj = nn.Linear(2 * embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, R_idx):\n",
    "        # x: (B, N, D)\n",
    "        B, N, D = x.shape\n",
    "        # We assume T * F = N (patch grid), R_idx: resolution one-hot or idx per token\n",
    "        time_pos = self.time_emb.unsqueeze(1)  # (T, 1, D)\n",
    "        freq_pos = self.freq_emb.unsqueeze(0)  # (1, F, D)\n",
    "        pe2d = (time_pos + freq_pos).reshape(-1, D)  # (T*F, D)\n",
    "\n",
    "        res_pos = self.res_emb[R_idx]  # (N, D)\n",
    "        pos = self.proj(torch.cat([pe2d[:N], res_pos], dim=-1))  # (N, D)\n",
    "        return x + pos.unsqueeze(0)\n",
    "\n",
    "class AcousticAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha_mel = nn.MultiheadAttention(embed_dim, num_heads // 2, batch_first=True)\n",
    "        self.mha_fft = nn.MultiheadAttention(embed_dim, num_heads // 2, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, filter_type_mask):\n",
    "        # x: (B, N, D), filter_type_mask: (B, N) → 0=mel, 1=fft\n",
    "        x_mel = x.clone()\n",
    "        x_fft = x.clone()\n",
    "\n",
    "        x_mel[filter_type_mask == 1] = 0  # mask out fft in mel head\n",
    "        x_fft[filter_type_mask == 0] = 0  # mask out mel in fft head\n",
    "\n",
    "        x1, _ = self.mha_mel(x_mel, x_mel, x_mel)\n",
    "        x2, _ = self.mha_fft(x_fft, x_fft, x_fft)\n",
    "\n",
    "        x_out = x + x1 + x2\n",
    "        x_out = self.norm(x_out)\n",
    "        return x_out + self.ffn(x_out)\n",
    "\n",
    "class AcousticTransformer(nn.Module):\n",
    "    def __init__(self, T=128, F=128, R=6, embed_dim=96, num_heads=4, num_layers=4, num_classes=13):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(patch_size=(8, 8), in_channels=R, embed_dim=embed_dim)\n",
    "        self.pos_enc = PositionalEncoding3D(T//8, F//8, R, embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            AcousticAttentionBlock(embed_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, F, R = x.shape  # <--- THIS IS THE FIX\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)  # (B, N, D)\n",
    "        # Dummy R indices assuming uniform resolution arrangement (for demo)\n",
    "        N = x.size(1)\n",
    "        R_idx = torch.arange(0, R).repeat((N // R) + 1)[:N].to(x.device)\n",
    "\n",
    "        # Dummy binary mask for filters (mel=0, fft=1)\n",
    "        filter_mask = (R_idx % 2).unsqueeze(0).repeat(B, 1)\n",
    "\n",
    "        x = self.pos_enc(x, R_idx)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, filter_mask)\n",
    "\n",
    "        x = x.mean(dim=1)  # Global average pooling over patches\n",
    "        return self.cls_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17e02350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting xMRMF features:   0%|          | 0/520 [00:00<?, ?it/s]/Users/anampavicic/miniconda3/envs/DLClass/lib/python3.11/site-packages/librosa/feature/spectral.py:2148: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n",
      "/Users/anampavicic/miniconda3/envs/DLClass/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=32 is too large for input signal of length=0\n",
      "  warnings.warn(\n",
      "/Users/anampavicic/miniconda3/envs/DLClass/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=64 is too large for input signal of length=0\n",
      "  warnings.warn(\n",
      "/Users/anampavicic/miniconda3/envs/DLClass/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=128 is too large for input signal of length=0\n",
      "  warnings.warn(\n",
      "Extracting xMRMF features: 100%|██████████| 520/520 [00:02<00:00, 223.58it/s]\n",
      "Extracting xMRMF features: 100%|██████████| 130/130 [00:00<00:00, 169.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.6406\n",
      "Epoch 2: Train Loss = 2.5692\n",
      "Epoch 3: Train Loss = 2.5348\n",
      "Epoch 4: Train Loss = 2.5242\n",
      "Epoch 5: Train Loss = 2.4825\n",
      "Epoch 6: Train Loss = 2.4156\n",
      "Epoch 7: Train Loss = 2.2216\n",
      "Epoch 8: Train Loss = 2.1723\n",
      "Epoch 9: Train Loss = 1.9923\n",
      "Epoch 10: Train Loss = 1.9342\n",
      "Epoch 11: Train Loss = 1.8029\n",
      "Epoch 12: Train Loss = 1.8051\n",
      "Epoch 13: Train Loss = 1.8540\n",
      "Epoch 14: Train Loss = 1.7603\n",
      "Epoch 15: Train Loss = 1.6826\n",
      "Epoch 16: Train Loss = 1.5986\n",
      "Epoch 17: Train Loss = 1.7506\n",
      "Epoch 18: Train Loss = 1.5985\n",
      "Epoch 19: Train Loss = 1.5784\n",
      "Epoch 20: Train Loss = 1.4928\n",
      "Epoch 21: Train Loss = 1.4699\n",
      "Epoch 22: Train Loss = 1.3910\n",
      "Epoch 23: Train Loss = 1.3293\n",
      "Epoch 24: Train Loss = 1.4455\n",
      "Epoch 25: Train Loss = 1.2997\n",
      "Epoch 26: Train Loss = 1.2864\n",
      "Epoch 27: Train Loss = 1.2060\n",
      "Epoch 28: Train Loss = 1.1737\n",
      "Epoch 29: Train Loss = 1.1524\n",
      "Epoch 30: Train Loss = 1.1485\n",
      "Epoch 31: Train Loss = 1.1772\n",
      "Epoch 32: Train Loss = 1.1105\n",
      "Epoch 33: Train Loss = 1.0440\n",
      "Epoch 34: Train Loss = 1.0699\n",
      "Epoch 35: Train Loss = 1.0076\n",
      "Epoch 36: Train Loss = 0.9823\n",
      "Epoch 37: Train Loss = 1.0108\n",
      "Epoch 38: Train Loss = 0.9651\n",
      "Epoch 39: Train Loss = 0.9519\n",
      "Epoch 40: Train Loss = 0.8518\n",
      "Epoch 41: Train Loss = 0.9167\n",
      "Epoch 42: Train Loss = 1.0135\n",
      "Epoch 43: Train Loss = 1.0150\n",
      "Epoch 44: Train Loss = 0.9549\n",
      "Epoch 45: Train Loss = 0.8800\n",
      "Epoch 46: Train Loss = 0.8822\n",
      "Epoch 47: Train Loss = 0.8151\n",
      "Epoch 48: Train Loss = 0.7164\n",
      "Epoch 49: Train Loss = 0.7137\n",
      "Epoch 50: Train Loss = 0.6875\n",
      "Epoch 51: Train Loss = 0.6876\n",
      "Epoch 52: Train Loss = 0.7152\n",
      "Epoch 53: Train Loss = 0.7680\n",
      "Epoch 54: Train Loss = 0.9463\n",
      "Epoch 55: Train Loss = 0.7374\n",
      "Epoch 56: Train Loss = 0.6503\n",
      "Epoch 57: Train Loss = 0.6149\n",
      "Epoch 58: Train Loss = 0.5727\n",
      "Epoch 59: Train Loss = 0.5472\n",
      "Epoch 60: Train Loss = 0.5246\n",
      "Epoch 61: Train Loss = 0.5617\n",
      "Epoch 62: Train Loss = 0.4752\n",
      "Epoch 63: Train Loss = 0.4937\n",
      "Epoch 64: Train Loss = 0.5043\n",
      "Epoch 65: Train Loss = 0.6541\n",
      "Epoch 66: Train Loss = 0.5327\n",
      "Epoch 67: Train Loss = 0.4678\n",
      "Epoch 68: Train Loss = 0.3754\n",
      "Epoch 69: Train Loss = 0.3676\n",
      "Epoch 70: Train Loss = 0.3140\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load dataset\n",
    "loader = DatasetLoader(csv_path='data/Animal_Sound.csv')\n",
    "train_data, val_data, input_dim, num_classes = loader.load_dataset()\n",
    "\n",
    "# Wrap into TensorDataset\n",
    "train_dataset = TensorDataset(train_data[0], train_data[1])  # [data, labels]\n",
    "val_dataset = TensorDataset(val_data[0], val_data[1])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "model = AcousticTransformer(\n",
    "    T=input_dim[0],\n",
    "    F=input_dim[1],\n",
    "    R=input_dim[2],\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# Optimizer & loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(70):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "126399d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 67.69%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "acc = correct / total * 100\n",
    "print(f\"Validation Accuracy: {acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

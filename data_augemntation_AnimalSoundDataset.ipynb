{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb5e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Libraries for processing sounds\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import random\n",
    "\n",
    "def extract_mel_spectrogram(file_path, sr=22050, n_mels=128):\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    return mel_db\n",
    "\n",
    "def pad_or_trim(mel, target_width=400):\n",
    "    if mel.shape[1] > target_width:\n",
    "        mel = mel[:, :target_width]\n",
    "    elif mel.shape[1] < target_width:\n",
    "        pad_width = target_width - mel.shape[1]\n",
    "        mel = np.pad(mel, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    return mel\n",
    "\n",
    "class AnimalSoundDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, split='train', split_ratio=0.8, seed=42):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "        all_paths = []\n",
    "        all_labels = []\n",
    "\n",
    "        for idx, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            for file_name in os.listdir(class_path):\n",
    "                if file_name.endswith(\".wav\"):\n",
    "                    all_paths.append(os.path.join(class_path, file_name))\n",
    "                    all_labels.append(idx)\n",
    "\n",
    "        # Shuffle and split\n",
    "        combined = list(zip(all_paths, all_labels))\n",
    "        random.seed(seed)\n",
    "        random.shuffle(combined)\n",
    "        split_point = int(len(combined) * split_ratio)\n",
    "\n",
    "        if split == 'train':\n",
    "            selected = combined[:split_point]\n",
    "        elif split == 'val':\n",
    "            selected = combined[split_point:]\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train' or 'val'\")\n",
    "\n",
    "        self.file_paths, self.labels = zip(*selected) if selected else ([], [])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = extract_mel_spectrogram(self.file_paths[idx])  # [n_mels, time]\n",
    "        mel = pad_or_trim(mel, target_width=400)\n",
    "        mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)  # [1, H, W]\n",
    "        label = self.labels[idx]\n",
    "        return mel, label\n",
    "    \n",
    "        \n",
    "    def get_class(self,idx):\n",
    "        label = self.labels[idx]\n",
    "        return self.classes[label]\n",
    "    \n",
    "    def visualize(self,n):\n",
    "    #print(dataset[n][0].squeeze(0))\n",
    "        plt.figure(figsize=(16,6))\n",
    "        librosa.display.specshow(\n",
    "                            self[n][0].squeeze(0).numpy(),\n",
    "                            x_axis=\"time\",\n",
    "                            y_axis=\"mel\")\n",
    "        plt.colorbar()\n",
    "\n",
    "    def play(self,n):\n",
    "        path = self.file_paths[n]\n",
    "        #print(path)\n",
    "        x, Fs = librosa.load(path, sr=None)\n",
    "        label = self.get_class(n)\n",
    "        print('Class: {}'.format(label))\n",
    "        return Audio(x, rate=Fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb68de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_path(path, new_path):\n",
    "    train_csv = pd.read_csv(path)\n",
    "    train_csv['path']=new_path\n",
    "    train_csv['class'] = train_csv['name'].apply(lambda x: x.split('_')[0])\n",
    "    train_csv.to_csv('data.csv')\n",
    "    \n",
    "    \n",
    "    \n",
    "path = 'Animal_Sound.csv'\n",
    "new_path = 'Animal-Soundprepros/'\n",
    "change_path(path, new_path=new_path)\n",
    "\n",
    "train_csv = pd.read_csv(path)\n",
    "\n",
    "dataset_train = AnimalSoundDataset(new_path, split='train', split_ratio=0.8, seed=42)\n",
    "dataset_val = AnimalSoundDataset(new_path, split='val', split_ratio=0.8, seed=42)\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "loader = DataLoader(dataset_train, batch_size=len(dataset_train))\n",
    "x_train, y_train = next(iter(loader))\n",
    "\n",
    "loader = DataLoader(dataset_val, batch_size=len(dataset_val))\n",
    "x_val, y_val = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621a0c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([520, 1, 128, 400])\n",
      "torch.Size([520])\n",
      "torch.Size([130, 1, 128, 400])\n",
      "torch.Size([130])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bca8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_log_mel_spectrogram(\n",
    "    filepath,\n",
    "    sr=22050,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    "):\n",
    "    try:\n",
    "        y, _ = librosa.load(filepath, sr=sr)\n",
    "\n",
    "        # Check if audio is silent or invalid\n",
    "        if not np.isfinite(y).all() or np.max(np.abs(y)) == 0:\n",
    "            raise ValueError(\"Invalid or silent audio\")\n",
    "\n",
    "        # Normalize safely\n",
    "        y = y / np.max(np.abs(y))\n",
    "\n",
    "        # Compute mel spectrogram\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=y,\n",
    "            sr=sr,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "        )\n",
    "\n",
    "        # Convert to log scale\n",
    "        log_mel = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "        # Final safety check\n",
    "        if not np.isfinite(log_mel).all():\n",
    "            raise ValueError(\"NaN or inf in spectrogram\")\n",
    "\n",
    "        return log_mel\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Failed to extract mel from {filepath}: {e}\")\n",
    "        # Return a zero tensor of expected shape\n",
    "        return np.zeros((n_mels, 400), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246cf6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Libraries for processing sounds\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import random\n",
    "\n",
    "\n",
    "def extract_mel_spectrogram(file_path, sr=22050, n_mels=128):\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    return mel_db\n",
    "\n",
    "def pad_or_trim(mel, target_width=400):\n",
    "    if mel.shape[1] > target_width:\n",
    "        mel = mel[:, :target_width]\n",
    "    elif mel.shape[1] < target_width:\n",
    "        pad_width = target_width - mel.shape[1]\n",
    "        mel = np.pad(mel, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    return mel\n",
    "\n",
    "class AnimalSoundDataset_new(Dataset):\n",
    "    def __init__(self, data_path, transform=None, split='train', split_ratio=0.8, seed=42):\n",
    "        self.transform = transform\n",
    "        self.data_path = data_path\n",
    "\n",
    "        df = pd.read_csv(data_path)\n",
    "\n",
    "        all_paths = df['path']\n",
    "        all_labels = df['name']\n",
    "        self.classes = df['name'].unique().tolist()\n",
    "\n",
    "        # Build label-to-index map\n",
    "        self.class_to_idx = {label: idx for idx, label in enumerate(sorted(set(all_labels)))}\n",
    "        self.classes = list(self.class_to_idx.keys())  # e.g., ['cat', 'cow', 'dog']\n",
    "\n",
    "        # Encode labels\n",
    "        encoded_labels = [self.class_to_idx[label] for label in all_labels]\n",
    "\n",
    "        # Shuffle and split\n",
    "        combined = list(zip(all_paths, encoded_labels))\n",
    "        random.seed(seed)\n",
    "        random.shuffle(combined)\n",
    "        split_point = int(len(combined) * split_ratio)\n",
    "\n",
    "        if split == 'train':\n",
    "            selected = combined[:split_point]\n",
    "        elif split == 'val':\n",
    "            selected = combined[split_point:]\n",
    "        else:\n",
    "            raise ValueError(\"split must be 'train' or 'val'\")\n",
    "\n",
    "        self.file_paths, self.labels = zip(*selected) if selected else ([], [])\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # mel = extract_mel_spectrogram(self.file_paths[idx])  # [n_mels, time]\n",
    "        # mel = pad_or_trim(mel, target_width=400)\n",
    "        # mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)  # [1, H, W]\n",
    "        # label = self.labels[idx]\n",
    "        # return mel, label\n",
    "\n",
    "        mel_log = extract_log_mel_spectrogram(self.file_paths[idx])  # [n_mels, time]\n",
    "        mel_log = pad_or_trim(mel_log, target_width=400)\n",
    "        mel_log = torch.tensor(mel_log, dtype=torch.float32).unsqueeze(0)\n",
    "        label = self.labels[idx]\n",
    "        return mel_log, label\n",
    "  \n",
    "    def get_class(self,idx):\n",
    "        label = self.labels[idx]\n",
    "        return self.classes[label]\n",
    "    \n",
    "    def visualize(self,n):\n",
    "    #print(dataset[n][0].squeeze(0))\n",
    "        plt.figure(figsize=(16,6))\n",
    "        librosa.display.specshow(\n",
    "                            self[n][0].squeeze(0).numpy(),\n",
    "                            x_axis=\"time\",\n",
    "                            y_axis=\"mel\")\n",
    "        plt.colorbar()\n",
    "\n",
    "    def play(self,n):\n",
    "        path = self.file_paths[n]\n",
    "        #print(path)\n",
    "        x, Fs = librosa.load(path, sr=None)\n",
    "        label = self.get_class(n)\n",
    "        print('Class: {}'.format(label))\n",
    "        return Audio(x, rate=Fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d06486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Failed to extract mel from data/augmented/Lion_1_modified.wav: Invalid or silent audio\n",
      "[WARNING] Failed to extract mel from data/sounds/Lion_1.wav: zero-size array to reduction operation maximum which has no identity\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/Animal_Sound_modified.csv'\n",
    "dataset_train = AnimalSoundDataset_new(data_path, split='train', split_ratio=0.8, seed=42)\n",
    "dataset_val = AnimalSoundDataset_new(data_path, split='val', split_ratio=0.8, seed=42)\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "loader = DataLoader(dataset_train, batch_size=len(dataset_train))\n",
    "x_train, y_train = next(iter(loader))\n",
    "\n",
    "loader = DataLoader(dataset_val, batch_size=len(dataset_val))\n",
    "x_val, y_val = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda9b062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1040, 1, 128, 400])\n",
      "torch.Size([1040])\n",
      "torch.Size([260, 1, 128, 400])\n",
      "torch.Size([260])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
